# ğŸ§  Prompt Agent Framework

Agentic Loop zur Bewertung, Verbesserung und Versionierung von LLM-Prompts.

## ğŸš€ Was ist das?

Dieses Projekt automatisiert die QualitÃ¤tskontrolle und Verbesserung von YAML-basierten Prompts fÃ¼r Large Language Models (LLMs). Es nutzt agentenbasierte Komponenten, um Prompts schrittweise zu analysieren, Feedback zu geben und zu verbessern, bis ein definierter QualitÃ¤tsstandard erreicht ist.

## ğŸ§© Architektur

```mermaid
graph TD
    A[run_template_batch.py] --> B[PromptQualityAgent]
    B -->|issues.json| D[ControllerAgent]
    D --> E[PromptImprovementAgent]
    E -->|new_prompt.yaml| A
    D -->|retry| B
```

### Komponenten

| Agent                            | Aufgabe                                                                   |
| -------------------------------- | ------------------------------------------------------------------------- |
| **PromptQualityAgent**     | Bewertet Prompt-QualitÃ¤t Ã¼ber Metriken (z. B. task clarity, robustness) |
| **ControllerAgent**        | PrÃ¼ft Feedback auf VollstÃ¤ndigkeit und semantische Korrektheit via GPT  |
| **PromptImprovementAgent** | Wendet Verbesserungen auf YAML-Prompt an und dokumentiert sie             |

## ğŸ§ª BeispielausfÃ¼hrung

### Einzeldatei verarbeiten

```bash
python run_template_batch.py --file prompts/templates/feature_determination_v1.yaml
```

### Mehrere Templates im Batch-Modus

```bash
python run_template_batch.py --all
```

## ğŸ“ Verzeichnisstruktur

```
.
â”œâ”€â”€ prompts/templates/                  # Eingabe-Prompts (YAML, *_v1.yaml)
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ quality_log/                    # JSON: Dimension -> Issue
â”‚   â”œâ”€â”€ feedback_log/                   # Verbesserungsfeedback
â”‚   â”œâ”€â”€ change_log/                     # Prompt-Diffs mit Rationale
â”‚   â””â”€â”€ weighted_score/                 # Score-only Files
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ prompt_quality_agent.py
â”‚   â”œâ”€â”€ prompt_improvement_agent.py
â”œâ”€â”€ controller_agent.py
â”œâ”€â”€ run_template_batch.py
â””â”€â”€ .env                                # OPENAI_API_KEY
```

## âš™ï¸ Konfiguration

- Alle Score-Definitionen unter `config/scoring/quality_scoring_matrix.json`
- `.env` benÃ¶tigt:

```
OPENAI_API_KEY=sk-...
```

## âœ… Beispiel-Output

```json
{
  "task_clarity": "Minor or no issues detected in 'task_clarity'.",
  "output_spec": "The dimension 'output_spec' has room for improvement.",
  "robustness": "The dimension 'robustness' shows critical weaknesses and needs revision."
}
```

## ğŸ“Œ Hinweis

- Du kannst den Controller-Agent so konfigurieren, dass er nach bestimmten Versionen abbricht oder neue Varianten erzeugt.
- Prompt-QualitÃ¤t wird Ã¼ber gewichtete Scores berechnet und versioniert abgelegt.

## ğŸ“¬ Kontakt

Konstantin | Data Analyst & Retail Consultant

---

Letzte Aktualisierung: 2025-05-31
